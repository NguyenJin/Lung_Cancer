# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14uESv3ilYBgC5OhB9D2468Zs1J6fbRO8
"""

import tensorflow as tf
import cv2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import numpy as np
import os
from PIL import Image

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/DA004/LC_dataset/

train_dataset = 'train'
validation_dataset = 'valid'
test_dataset = 'test'

# Tham số tiền xử lý
img_width, img_height = 224, 224
batch_size = 32

# Tăng cường dữ liệu cho dữ liệu đào tạo
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2
)

# Không tăng cường dữ liệu cho dữ liệu xác thực
train_dataset = ImageDataGenerator(rescale=1/255)
validation_dataset = ImageDataGenerator(rescale=1/255)

# Bộ tạo dữ liệu
train_dataset = train_dataset.flow_from_directory(directory='train',
                                       target_size=(224, 224),
                                       batch_size=32,
                                       class_mode='binary')
validation_dataset = validation_dataset.flow_from_directory('valid',
                                             target_size=(224, 224),
                                             batch_size=32,
                                             class_mode='binary')

train_dataset.class_indices

validation_dataset.class_indices

model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    #
                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
                                    tf.keras.layers.MaxPooling2D(2,2),
                                    #
                                    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
                                    tf.keras.layers.MaxPooling2D(2,2),
                                    #
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(512, activation='relu'),
                                    ##
                                    tf.keras.layers.Dense(1, activation='sigmoid')
                                    ])

# # Biên dịch mô hình
model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)
history = model.fit(train_dataset,
                    steps_per_epoch=len(train_dataset),
                    epochs=20,
                    validation_data=validation_dataset)

# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# # Callbacks
# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)

# # Train mô hình
# history = model.fit(
#     train_generator,
#     steps_per_epoch=train_generator.samples // batch_size,
#     epochs=20,  # Tăng số epoch để xem Early Stopping có hiệu quả không
#     validation_data=validation_generator,
#     validation_steps=validation_generator.samples // batch_size,
#     callbacks=[early_stopping, reduce_lr]
# )

dir_path = 'test/Normal'
for i in os.listdir(dir_path):
    img = cv2.imread(os.path.join(dir_path, i))
    img = cv2.resize(img, (224, 224))
    img = np.expand_dims(img, axis=0)
    img = img / 255.0
    prediction = model.predict(img)
    val = model.predict(img)
    if val == 1:
        print("Binh thuong")
    else:
        print("Bat binh thuong")

dir_path = 'test/Bacterial Pneumonia/'
for i in os.listdir(dir_path):
    img = cv2.imread(os.path.join(dir_path, i))
    img = cv2.resize(img, (224, 224))
    img = np.expand_dims(img, axis=0)
    img = img / 255.0
    prediction = model.predict(img)
    val = model.predict(img)
    if val == 1:
        print("Binh thuong")
    else:
        print("Bat binh thuong")

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot g & validation loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Đánh giá mô hình trên tập kiểm tra
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    test_dataset,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary'
)


test_loss, test_acc = model.evaluate(test_generator)
print('Độ mất mát trên tập kiểm tra:', test_loss)
print('Độ chính xác trên tập kiểm tra:', test_acc)

# Lưu mô hình
model.save('/content/drive/MyDrive/DA004/model_LC.h5')
model.save('/content/drive/MyDrive/DA004/model_LC.keras')